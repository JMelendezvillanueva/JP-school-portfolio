{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQmsEIZn7pf_",
        "outputId": "4ca3004a-f7a0-4e4d-b45b-b0f686a3a803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['L9', 'L8', 'L5', 'L2', 'L1']\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Q-Learning using numpy.ipynb (.txt version)\n",
        "\n",
        "\n",
        "<center>The environment</center>\n",
        "\"\"\"\n",
        "\n",
        "# Only numpy\n",
        "import numpy as np\n",
        "\n",
        "# Initialize parameters\n",
        "gamma = 0.75 # Discount factor\n",
        "alpha = 0.9 # Learning rate\n",
        "\n",
        "# Define the states\n",
        "location_to_state = {\n",
        "    'L1' : 0,\n",
        "    'L2' : 1,\n",
        "    'L3' : 2,\n",
        "    'L4' : 3,\n",
        "    'L5' : 4,\n",
        "    'L6' : 5,\n",
        "    'L7' : 6,\n",
        "    'L8' : 7,\n",
        "    'L9' : 8\n",
        "}\n",
        "\n",
        "# Define the actions\n",
        "actions = [0,1,2,3,4,5,6,7,8]\n",
        "\n",
        "\"\"\"![](https://i.ibb.co/k4kgnQS/Capture.png)\n",
        "\n",
        "<center>Rewards' table</center>\n",
        "\"\"\"\n",
        "\n",
        "# Define the rewards\n",
        "rewards = np.array([[0,1,0,0,0,0,0,0,0],\n",
        "              [1,0,1,0,0,0,0,0,0],\n",
        "              [0,1,0,0,0,1,0,0,0],\n",
        "              [0,0,0,0,0,0,1,0,0],\n",
        "              [0,1,0,0,0,0,0,1,0],\n",
        "              [0,0,1,0,0,0,0,0,0],\n",
        "              [0,0,0,1,0,0,0,1,0],\n",
        "              [0,0,0,0,1,0,1,0,1],\n",
        "              [0,0,0,0,0,0,0,1,0]])\n",
        "\n",
        "# Maps indices to locations\n",
        "state_to_location = dict((state,location) for location,state in location_to_state.items())\n",
        "\n",
        "# Define the actions\n",
        "actions = [0,1,2,3,4,5,6,7,8]\n",
        "\n",
        "\"\"\"The following function is going to take two arguments:\n",
        "\n",
        "- starting location in the warehouse and\n",
        "- end location in the warehouse respectively\n",
        "\n",
        "It will return the optimal route for reaching the end location from the starting location in the form of an ordered list (containing the letters).\n",
        "\"\"\"\n",
        "\n",
        "def get_optimal_route(start_location,end_location):\n",
        "    # Copy the rewards matrix to new Matrix\n",
        "    rewards_new = np.copy(rewards)\n",
        "    # Get the ending state corresponding to the ending location as given\n",
        "    ending_state = location_to_state[end_location]\n",
        "    # With the above information automatically set the priority of the given ending state to the highest one\n",
        "    rewards_new[ending_state,ending_state] = 999\n",
        "\n",
        "    # -----------Q-Learning algorithm-----------\n",
        "\n",
        "    # Initializing Q-Values\n",
        "    Q = np.array(np.zeros([9,9]))\n",
        "\n",
        "    # Q-Learning process\n",
        "    for i in range(1000):\n",
        "        # Pick up a state randomly\n",
        "        current_state = np.random.randint(0,9) # Python excludes the upper bound\n",
        "        # For traversing through the neighbor locations in the maze\n",
        "        playable_actions = []\n",
        "        # Iterate through the new rewards matrix and get the actions > 0\n",
        "        for j in range(9):\n",
        "            if rewards_new[current_state,j] > 0:\n",
        "                playable_actions.append(j)\n",
        "        # Pick an action randomly from the list of playable actions leading us to the next state\n",
        "        next_state = np.random.choice(playable_actions)\n",
        "        # Compute the temporal difference\n",
        "        # The action here exactly refers to going to the next state\n",
        "        TD = rewards_new[current_state,next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state,next_state]\n",
        "        # Update the Q-Value using the Bellman equation\n",
        "        Q[current_state,next_state] += alpha * TD\n",
        "\n",
        "    # Initialize the optimal route with the starting location\n",
        "    route = [start_location]\n",
        "    # We do not know about the next location yet, so initialize with the value of starting location\n",
        "    next_location = start_location\n",
        "\n",
        "    # We don't know about the exact number of iterations needed to reach to the final location hence while loop will be a good choice for iteratiing\n",
        "    while(next_location != end_location):\n",
        "        # Fetch the starting state\n",
        "        starting_state = location_to_state[start_location]\n",
        "        # Fetch the highest Q-value pertaining to starting state\n",
        "        next_state = np.argmax(Q[starting_state,])\n",
        "        # We got the index of the next state. But we need the corresponding letter.\n",
        "        next_location = state_to_location[next_state]\n",
        "        route.append(next_location)\n",
        "        # Update the starting location for the next iteration\n",
        "        start_location = next_location\n",
        "\n",
        "    return route\n",
        "\n",
        "print(get_optimal_route('L9', 'L1'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimal_route(start_location,end_location):\n",
        "    ...\n",
        "    route = [start_location]\n",
        "    next_location = start_location\n",
        "    steps = 0  # Track number of steps\n",
        "\n",
        "    while(next_location != end_location):\n",
        "        starting_state = location_to_state[start_location]\n",
        "        next_state = np.argmax(Q[starting_state,])\n",
        "        next_location = state_to_location[next_state]\n",
        "        route.append(next_location)\n",
        "        start_location = next_location\n",
        "        steps += 1  # Increment steps\n",
        "\n",
        "    return route, steps\n"
      ],
      "metadata": {
        "id": "WseRcPe_79ZX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_q_learning(rewards_matrix, gamma=0.75, alpha=0.9, iterations=1000):\n",
        "    Q = np.zeros_like(rewards_matrix)\n",
        "    for _ in range(iterations):\n",
        "        current_state = np.random.randint(0, rewards_matrix.shape[0])\n",
        "        playable_actions = np.where(rewards_matrix[current_state] > 0)[0]\n",
        "        next_state = np.random.choice(playable_actions)\n",
        "        TD = rewards_matrix[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state])] - Q[current_state, next_state]\n",
        "        Q[current_state, next_state] += alpha * TD\n",
        "    return Q\n"
      ],
      "metadata": {
        "id": "au2KV8gq8Ig9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimal_route(start_location, end_location, Q):\n",
        "    route = [start_location]\n",
        "    next_location = start_location\n",
        "    steps = 0\n",
        "    while next_location != end_location:\n",
        "        starting_state = location_to_state[start_location]\n",
        "        next_state = np.argmax(Q[starting_state])\n",
        "        next_location = state_to_location[next_state]\n",
        "        route.append(next_location)\n",
        "        start_location = next_location\n",
        "        steps += 1\n",
        "    return route, steps\n"
      ],
      "metadata": {
        "id": "6UNoV4X98LYn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Q\n",
        "Q = train_q_learning(rewards)\n",
        "\n",
        "# Get route\n",
        "route, steps = get_optimal_route('L9', 'L1', Q)\n",
        "print(\"Route:\", route)\n",
        "print(\"Steps:\", steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIu15uH68Mt-",
        "outputId": "3e52d5c3-1481-4a38-ac81-8256d18c386f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route: ['L9', 'L1']\n",
            "Steps: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Reward L9 to L1:\", rewards[8, 0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGySZFHb8Zpp",
        "outputId": "458162e1-816b-46f6-cff1-3c6e156467f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward L9 to L1: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = np.array([\n",
        "    [0,1,0,0,0,0,0,0,0],\n",
        "    [1,0,1,0,0,0,0,0,0],\n",
        "    [0,1,0,0,0,1,0,0,0],\n",
        "    [0,0,0,0,0,0,1,0,0],\n",
        "    [0,1,0,0,0,0,0,1,0],\n",
        "    [0,0,1,0,0,0,0,0,0],\n",
        "    [0,0,0,1,0,0,0,1,0],\n",
        "    [0,0,0,0,1,0,1,0,1],\n",
        "    [0,0,0,0,0,0,0,1,0]\n",
        "])\n"
      ],
      "metadata": {
        "id": "M7hrbLA98c8k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = train_q_learning(rewards, gamma=0.75, alpha=0.9)\n",
        "route, steps = get_optimal_route('L9', 'L1', Q)\n",
        "print(\"Route:\", route)\n",
        "print(\"Steps:\", steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeJ5y3448jQD",
        "outputId": "65ff072a-2dec-4b25-d586-9a122237c8cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route: ['L9', 'L1']\n",
            "Steps: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"L9 to L1:\", rewards[8, 0])\n",
        "print(\"L1 to L9:\", rewards[0, 8])\n",
        "print(\"L9 connections:\", rewards[8])\n",
        "print(\"L1 connections:\", rewards[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8s_E2ow8pNS",
        "outputId": "00d2ab6d-233f-4665-da5a-2077142e05d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L9 to L1: 0\n",
            "L1 to L9: 0\n",
            "L9 connections: [0 0 0 0 0 0 0 1 0]\n",
            "L1 connections: [0 1 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = np.array([\n",
        "    [0,1,0,0,0,0,0,0,0],\n",
        "    [1,0,1,0,0,0,0,0,0],\n",
        "    [0,1,0,0,0,1,0,0,0],\n",
        "    [0,0,0,0,0,0,1,0,0],\n",
        "    [0,1,0,0,0,0,0,1,0],\n",
        "    [0,0,1,0,0,0,0,0,0],\n",
        "    [0,0,0,1,0,0,0,1,0],\n",
        "    [0,0,0,0,1,0,1,0,1],\n",
        "    [0,0,0,0,0,0,0,1,0]\n",
        "])\n"
      ],
      "metadata": {
        "id": "XTLOOZ5F8txD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = train_q_learning(rewards, gamma=0.75, alpha=0.9)\n",
        "route, steps = get_optimal_route('L9', 'L1', Q)\n",
        "print(\"Route:\", route)\n",
        "print(\"Steps:\", steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4mhm8Jq8u8_",
        "outputId": "f4603b89-899c-42e8-f93b-e41e7d9eaed5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route: ['L9', 'L1']\n",
            "Steps: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q[8]:\", Q[8])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YGQN--Q8w9Y",
        "outputId": "8a9081e6-295d-4998-dc01-ff96392d638c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q[8]: [0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"L9 to L1:\", rewards[8, 0])\n",
        "print(\"Q[8]:\", Q[8])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feJAA_LD8ztT",
        "outputId": "dceb07ba-a4a8-432e-c63a-ebc3aa49a09d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L9 to L1: 0\n",
            "Q[8]: [0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_q_learning_with_goal(rewards_matrix, start_location, end_location, gamma=0.75, alpha=0.9, iterations=1000):\n",
        "    rewards_new = np.copy(rewards_matrix)\n",
        "    ending_state = location_to_state[end_location]\n",
        "    rewards_new[ending_state, ending_state] = 999\n",
        "\n",
        "    Q = np.zeros_like(rewards_matrix)\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        current_state = np.random.randint(0, rewards_matrix.shape[0])\n",
        "        playable_actions = np.where(rewards_new[current_state] > 0)[0]\n",
        "        if len(playable_actions) == 0:\n",
        "            continue  # skip if no valid actions\n",
        "        next_state = np.random.choice(playable_actions)\n",
        "        TD = rewards_new[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state])] - Q[current_state, next_state]\n",
        "        Q[current_state, next_state] += alpha * TD\n",
        "\n",
        "    return Q\n"
      ],
      "metadata": {
        "id": "zuOATq5O83n1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = train_q_learning_with_goal(rewards, start_location='L9', end_location='L1')\n",
        "route, steps = get_optimal_route('L9', 'L1', Q)\n",
        "print(\"Route:\", route)\n",
        "print(\"Steps:\", steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix-mDYke8_D8",
        "outputId": "9d2e80ef-3ac1-4b4d-a58c-09763a6589ae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route: ['L9', 'L8', 'L5', 'L2', 'L1']\n",
            "Steps: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q = train_q_learning_with_goal(rewards, start_location='L9', end_location='L1', iterations=50)\n",
        "route, steps = get_optimal_route('L9', 'L1', Q)\n",
        "print(\"50 Iterations → Route:\", route)\n",
        "print(\"Steps:\", steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jZLMkll9DdC",
        "outputId": "4dad326a-dc39-4ae6-de90-9b1f3589f8f8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 Iterations → Route: ['L9', 'L8', 'L5', 'L2', 'L1']\n",
            "Steps: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q = train_q_learning_with_goal(rewards, start_location='L9', end_location='L1', iterations=200)\n",
        "route, steps = get_optimal_route('L9', 'L1', Q)\n",
        "print(\"200 Iterations → Route:\", route)\n",
        "print(\"Steps:\", steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq664os_9E3A",
        "outputId": "6fab1335-38c2-4019-8ddd-a9aa8dd6d04f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 Iterations → Route: ['L9', 'L8', 'L5', 'L2', 'L1']\n",
            "Steps: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards[1, 4] = 1  # L2 → L5\n",
        "rewards[4, 1] = 1  # L5 → L2 (make it bidirectional)\n"
      ],
      "metadata": {
        "id": "cXDBYYCU9iur"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = train_q_learning_with_goal(rewards, start_location='L1', end_location='L9')\n",
        "route, steps = get_optimal_route('L1', 'L9', Q)\n",
        "print(\"Route:\", route)\n",
        "print(\"Steps:\", steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6S8baX19O5q",
        "outputId": "ca8eee0e-c81c-4508-e941-106b8322940b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route: ['L1', 'L2', 'L5', 'L8', 'L9']\n",
            "Steps: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = np.array([\n",
        "    [0,1,0,0,0,0,0,0,0,0],  # L1\n",
        "    [1,0,1,0,1,0,0,0,0,0],  # L2 (patched bidirectionally to L5)\n",
        "    [0,1,0,0,0,1,0,0,0,0],  # L3\n",
        "    [0,0,0,0,0,0,1,0,0,0],  # L4\n",
        "    [0,1,0,0,0,0,0,1,0,0],  # L5\n",
        "    [0,0,1,0,0,0,0,0,0,0],  # L6\n",
        "    [0,0,0,1,0,0,0,1,0,0],  # L7\n",
        "    [0,0,0,0,1,0,1,0,1,0],  # L8\n",
        "    [0,0,0,0,0,0,0,1,0,1],  # L9\n",
        "    [0,0,0,0,0,0,0,0,1,0],  # L10 → L9\n",
        "])\n"
      ],
      "metadata": {
        "id": "lcIxiImk9ro8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_to_state = {\n",
        "    'L1' : 0, 'L2' : 1, 'L3' : 2, 'L4' : 3, 'L5' : 4,\n",
        "    'L6' : 5, 'L7' : 6, 'L8' : 7, 'L9' : 8, 'L10': 9\n",
        "}\n",
        "state_to_location = dict((state, location) for location, state in location_to_state.items())\n"
      ],
      "metadata": {
        "id": "xUhf4TiR9so5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = train_q_learning_with_goal(rewards, start_location='L10', end_location='L1')\n",
        "route, steps = get_optimal_route('L10', 'L1', Q)\n",
        "print(\"L10 to L1 →\", route)\n",
        "print(\"Steps:\", steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4XVgGmJ9t7X",
        "outputId": "ee13c535-94b9-4285-9a37-5f19cacf0801"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L10 to L1 → ['L10', 'L9', 'L8', 'L5', 'L2', 'L1']\n",
            "Steps: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q = train_q_learning_with_goal(rewards, start_location='L10', end_location='L4')\n",
        "route, steps = get_optimal_route('L10', 'L4', Q)\n",
        "print(\"L10 to L4 →\", route)\n",
        "print(\"Steps:\", steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU3q7fSP9vkW",
        "outputId": "98da0d4e-6946-4725-8912-6b0b746cedef"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L10 to L4 → ['L10', 'L9', 'L8', 'L7', 'L4']\n",
            "Steps: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Q with very low gamma and alpha\n",
        "Q_low = train_q_learning_with_goal(rewards, start_location='L9', end_location='L1', gamma=0.05, alpha=0.05)\n",
        "\n",
        "# Get the route using the poorly trained Q-table\n",
        "route, steps = get_optimal_route('L9', 'L1', Q_low)\n",
        "print(\"Route with gamma=0.05 and alpha=0.05:\", route)\n",
        "print(\"Steps:\", steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb9O-U8wAjnW",
        "outputId": "58d56920-9d1b-4c67-9304-bef0ab9eadc7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route with gamma=0.05 and alpha=0.05: ['L9', 'L1']\n",
            "Steps: 1\n"
          ]
        }
      ]
    }
  ]
}